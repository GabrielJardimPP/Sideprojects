{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(n1, n2, low_x, high_x, low_t, high_t):\n",
    "    x = np.random.uniform(low = low_x, high = high_x, size = (n1+n2,1))\n",
    "    t = np.random.uniform(low = low_t, high = high_t, size = (n1,1))\n",
    "    t_0 = np.zeros((n2,1))\n",
    "    return x, np.vstack((t,t_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1, m2, low_x, high_x, low_t, high_x = 5000,5000,-2,2,0,2 \n",
    "x_train, t_train = gen_data(m1, m2, low_x, high_x, low_t, high_x)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           100000, 0.96, staircase=True)\n",
    "\n",
    "epochs = 200\n",
    "h_size = 50\n",
    "\n",
    "C = tf.constant(.2, dtype=tf.float32)\n",
    "\n",
    "# declare the training data placeholders\n",
    "x = tf.placeholder(tf.float32, [None, 1])\n",
    "t = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# concatenating then as inputs\n",
    "D = tf.concat([t,x], 1)\n",
    "\n",
    "# now declare the weights connecting the input to the first layer\n",
    "W_1 = tf.Variable(tf.random_normal([2, h_size], stddev=1/2), name='W_1')\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1/2), name='b_1')\n",
    "\n",
    "U_z1 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_z1')\n",
    "W_z1 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_z1')\n",
    "b_z1 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_z1')\n",
    "\n",
    "U_g1 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_g1')\n",
    "W_g1 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_g1')\n",
    "b_g1 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_g1')\n",
    "\n",
    "U_r1 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_r1')\n",
    "W_r1 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_r1')\n",
    "b_r1 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_r1')\n",
    "\n",
    "U_h1 = tf.Variable(tf.random_normal([2, h_size], stddev=0.03), name='U_h1')\n",
    "W_h1 = tf.Variable(tf.random_normal([h_size, h_size], stddev=0.03), name='W_h1')\n",
    "b_h1 = tf.Variable(tf.random_normal([h_size]), name='b_h1')\n",
    "\n",
    "\n",
    "# now declare the weights connecting the input to the second layer\n",
    "U_z2 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_z2')\n",
    "W_z2 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_z2')\n",
    "b_z2 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_z2')\n",
    "\n",
    "U_g2 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_g2')\n",
    "W_g2 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_g2')\n",
    "b_g2 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_g2')\n",
    "\n",
    "U_r2 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_r2')\n",
    "W_r2 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_r2')\n",
    "b_r2 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_r2')\n",
    "\n",
    "U_h2 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_h2')\n",
    "W_h2 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_h2')\n",
    "b_h2 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_h2')\n",
    "\n",
    "\n",
    "# and the third, final layer\n",
    "U_z3 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_z3')\n",
    "W_z3 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_z3')\n",
    "b_z3 = tf.Variable(tf.random_normal([h_size],stddev=1/(h_size+2)), name='b_z3')\n",
    "\n",
    "U_g3 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_g3')\n",
    "W_g3 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_g3')\n",
    "b_g3 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_g3')\n",
    "\n",
    "U_r3 = tf.Variable(tf.random_normal([2, h_size], stddev=1/(h_size+2)), name='U_r3')\n",
    "W_r3 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/(h_size+2)), name='W_r3')\n",
    "b_r3 = tf.Variable(tf.random_normal([h_size], stddev=1/(h_size+2)), name='b_r3')\n",
    "\n",
    "U_h3 = tf.Variable(tf.random_normal([2, h_size], stddev=0.5), name='U_h3')\n",
    "W_h3 = tf.Variable(tf.random_normal([h_size, h_size], stddev=1/h_size), name='W_h3')\n",
    "b_h3 = tf.Variable(tf.random_normal([h_size], stddev=1/h_size), name='b_h3')\n",
    "\n",
    "# finally, we compute the final weights\n",
    "W = tf.Variable(tf.random_normal([h_size, 1], stddev=1/h_size), name='W')\n",
    "b = tf.Variable(tf.random_normal([1], stddev=1/h_size), name='b')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We now proceed to make the computation graph:\n",
    "# first layer:\n",
    "S_1 = tf.tanh(tf.add(tf.matmul(D,W_1),b_1))\n",
    "Z_1 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_z1), tf.matmul(S_1, W_z1)), b_z1))\n",
    "G_1 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_g1), tf.matmul(S_1, W_g1)), b_g1))\n",
    "R_1 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_r1), tf.matmul(S_1, W_r1)), b_r1))\n",
    "H_1 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_h1), tf.matmul(tf.multiply(S_1, R_1), W_h1)), b_h1))\n",
    "\n",
    "# second layer:\n",
    "S_2 = tf.add(tf.multiply(tf.subtract(tf.ones_like(G_1), G_1), H_1), tf.multiply(Z_1,S_1))\n",
    "Z_2 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_z2), tf.matmul(S_2, W_z2)), b_z2))\n",
    "G_2 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_g2), tf.matmul(S_1, W_g2)), b_g2))\n",
    "R_2 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_r2), tf.matmul(S_2, W_r2)), b_r2))\n",
    "H_2 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_h2), tf.matmul(tf.multiply(S_2, R_2), W_h2)), b_h2))\n",
    "\n",
    "# third layer:\n",
    "S_3 = tf.add(tf.multiply(tf.subtract(tf.ones_like(G_2), G_2), H_2), tf.multiply(Z_2,S_2))\n",
    "Z_3 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_z3), tf.matmul(S_3, W_z3)), b_z3))\n",
    "G_3 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_g3), tf.matmul(S_1, W_g3)), b_g3))\n",
    "R_3 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_r3), tf.matmul(S_3, W_r3)), b_r3))\n",
    "H_3 = tf.tanh(tf.add(tf.add(tf.matmul(D,U_h3), tf.matmul(tf.multiply(S_3, R_3), W_h3)), b_h3))\n",
    "\n",
    "\n",
    "# output layer\n",
    "S_4 = tf.add(tf.multiply(tf.subtract(tf.ones_like(G_3), G_3), H_3), tf.multiply(Z_3, S_3))\n",
    "\n",
    "fhat = tf.add(tf.matmul(S_4, W), b)\n",
    "\n",
    "\n",
    "# in order to calculate the loss, we will need the derivative of the output function with respect to the input\n",
    "df_dx = tf.gradients(fhat,x)[0][0:m1]\n",
    "df_dt = tf.gradients(fhat,t)[0][0:m1]\n",
    "\n",
    "# we also need to consider our starting conditions:\n",
    "u_start_hat = tf.cast(tf.exp(-tf.square(x_train[m1:])),tf.float32)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# now, we create our cost function\n",
    "J = tf.add(tf.reduce_mean(tf.square(tf.add(df_dt, tf.multiply(C,df_dx)))), \n",
    "            tf.reduce_mean(tf.square(tf.subtract(u_start_hat, fhat[m1:]))))\n",
    "\n",
    "# J = tf.reduce_mean(tf.square(tf.add(df_dt, tf.multiply(C,df_dx))))\n",
    "\n",
    "# and add an optimizer\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(J)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        _, j = sess.run([optimiser, J], \n",
    "                     feed_dict={x: x_train, t: t_train})\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{}\".format(j))\n",
    "        \n",
    "    x_test = np.linspace(-2, 2,300).reshape(300,1)\n",
    "    t_test = 0.25*np.ones((300,1))\n",
    "    \n",
    "    f_test = sess.run(fhat, feed_dict={x: x_test, t: t_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test,f_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
